default:
batch size: 32
chunk_size: 4
n_chunk_per_traj: 4
lambda_commit: 0.02


# train_motion_vqvae_2.yaml
compression rate: 8
scaled up already, but still not that good

# train_motion_vqvae_3.yaml
try larger model from proprior config, but only 4 times compression rate
this looks much better. Now further scale up the model



# train_motion_vqvae_4.yaml
try encoder/decoder with transformer models.

# train_motion_vqvae_5.yaml
smaller transformer model than train_motion_vqvae_4.yaml
not as good as 4

# train_motion_vqvae_6.yaml
same as train_motion_vqvae_4.yaml, just that load trajectory of size 8 instead of 16
much worse than 4

# currently 4, 5, 6 are all worse than 3 in terms of recon loss.





# train_motion_vqvae_7.yaml
source from train_motion_vqvae_3.yaml, but change to cross entropy loss
gripper_weight: 0.1

# train_motion_vqvae_8.yaml
gripper_weight: 0.01

# train_motion_vqvae_9.yaml
gripper_weight: 0.5



# train_motion_vqvae_10.yaml
add vision information
really cannot train
this one also doesn't work when only 1 code for each whole chunk, similar to v16



# train_motion_vqvae_11.yaml
try larger codebook of size 1024

# train_motion_vqvae_12.yaml
try larger codebook of size 1536

# train_motion_vqvae_13.yaml
try larger codebook of size 2048



# train_motion_vqvae_14.yaml
from train_motion_vqvae_8.yaml
load sparce trajectory


# train_motion_vqvae_15.yaml
from train_motion_vqvae_14.yaml
add language information
the loss seems worse than 14



# train_motion_vqvae_16.yaml
from train_motion_vqvae_10.yaml
change to chunk size 8, 1 chunk per traj, 8 compression rate
also change to 1 gpu training, since multi-gpu training is a bit problematic
the reconstruction seems not bad for a single chunk. But when reconstructing the whole trajectory, it totally doesn't work
We can also observe that, rgb observation is indeed helpful


# train_motion_vqvae_17.yaml
from train_motion_vqvae_14.yaml
add another codebook layer to see whether it gets better



# train_motion_vqvae_18.yaml
from v10
use chunk size 4, 4 chunk per traj, but only one observation per chunk


# train_motion_vqvae_19.yaml
from v18
use new dataloader


# train_motion_vqvae_20.yaml
from v19
but use one chunk per traj
totally doesn't work
so I change to 4 layers of codebook


# train_motion_vqvae_21.yaml
from 19
use dataset v3: use fixed length of 16 actions, or 4 tokens, to represent each keypoint